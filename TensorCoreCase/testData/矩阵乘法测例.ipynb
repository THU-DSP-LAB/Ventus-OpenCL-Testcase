{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3cc9af9-fd02-4320-8f3a-251ff5d1fe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix C (FP16 on CUDA):\n",
      "tensor([[ 1.4932,  7.8789, -2.9883,  6.3281, -4.3945,  3.2383, -2.9805,  2.1914],\n",
      "        [-0.7417, -2.3086, -0.2527, -0.5068, -3.4180,  0.2129,  0.8003, -2.1016],\n",
      "        [-3.3945, -3.0586,  3.0586,  2.8594, -0.4517,  5.5586,  1.7969, -3.8008],\n",
      "        [ 4.9414, -4.0664, -4.5547, -0.8662, -0.4897,  2.0332, -1.0693, -4.5273],\n",
      "        [-0.5723, -1.6582, -6.1953,  1.5479, -0.8701, -0.5913, -1.5879,  2.5977],\n",
      "        [ 3.5586, -1.3086, -2.4258,  3.7031, -4.3477, -1.0762, -2.2852,  1.8350],\n",
      "        [-1.9014, -5.5703,  1.3418,  2.2441, -0.9556, -6.0820,  1.8818,  3.1250],\n",
      "        [ 4.2891, -3.6250, -4.1445,  1.0293, -3.5957, -3.5527, -0.0591,  3.3184],\n",
      "        [ 0.7393,  8.2422, -3.7500,  1.3018, -2.5332,  6.7500, -5.4844, -2.2188],\n",
      "        [-0.3381,  0.9902,  5.2656, -2.6719,  3.8184,  3.4219,  4.8086, -0.4141],\n",
      "        [ 1.6064, -4.6680,  2.9766, -6.3984,  0.0771,  1.5537,  0.9775, -5.3906],\n",
      "        [ 2.5898,  5.3828, -3.3672, -1.4668, -1.3936,  2.9551, -3.6387,  0.0447],\n",
      "        [-4.6094, -3.0195,  5.9961, -1.9297,  1.6816, -6.8828,  5.4648, -1.4414],\n",
      "        [ 1.2930, -0.2075,  0.9385, -1.3174,  0.8540,  1.3633, -0.1649, -1.3945],\n",
      "        [ 2.9746,  0.3650,  0.1641, -1.7959,  0.1582,  3.5098, -0.8765, -3.0391],\n",
      "        [-0.6680, 10.0156,  5.5078,  3.0137,  3.5586,  0.4241, -0.7339, -1.9707]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设使用FP16精度\n",
    "torch.set_default_dtype(torch.float16)\n",
    "\n",
    "# 创建矩阵A, B, D，并假设它们已经在GPU上\n",
    "A = torch.randn(16, 8, device='cuda')\n",
    "B = torch.randn(8, 8, device='cuda')\n",
    "D = torch.randn(16, 8, device='cuda')\n",
    "\n",
    "# 使用CUDA进行矩阵乘法 C = A * B\n",
    "C = torch.matmul(A, B)\n",
    "\n",
    "# 执行矩阵加法 C = C + D\n",
    "C = C + D\n",
    "\n",
    "# 如果需要将结果C移回CPU，可以这样做：\n",
    "C_cpu = C.cpu()\n",
    "\n",
    "# 打印结果以验证\n",
    "print(\"Matrix C (FP16 on CUDA):\")\n",
    "print(C_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be1cf0e2-de1c-4d3a-a607-423af9435ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6221,  2.0879,  0.0956,  0.8218, -0.9561, -0.1516,  1.9365, -0.0624],\n",
      "        [ 0.8413, -0.0354,  1.0918, -0.9727,  0.6768, -1.2070, -0.5635, -1.8213],\n",
      "        [ 0.7544, -0.1205, -0.2488,  0.1873, -0.8584,  0.2036, -0.2035,  0.1149],\n",
      "        [ 1.2441,  0.8169, -0.2462,  0.4360, -0.6348, -0.2815,  0.0539,  1.3076],\n",
      "        [-0.3623,  0.5576,  0.4390, -0.3479,  1.2715, -0.3364,  0.9175, -0.0255],\n",
      "        [-1.0303,  0.4353,  2.0430,  1.0342, -0.1237, -0.3501,  0.6016,  0.4126],\n",
      "        [ 2.0996, -0.2260,  0.7910,  0.8428,  0.8516,  0.5503,  0.5938, -1.4502],\n",
      "        [-0.7832,  0.5044,  0.4968, -0.4678,  0.7192, -1.0879,  0.1075, -2.0059],\n",
      "        [ 0.9409,  0.2189, -0.1530, -0.5674, -1.2119, -0.7217,  0.4043,  1.3789],\n",
      "        [-0.0617, -0.2238,  1.2197, -0.3728, -0.0536, -0.6187, -0.1010,  0.4390],\n",
      "        [-1.3262, -0.0433,  1.0439, -0.1665, -1.2002,  1.5645, -1.2676,  0.8877],\n",
      "        [-0.0344,  0.1157, -0.6284, -1.5693, -0.3877,  1.5234,  0.6577,  0.1648],\n",
      "        [ 0.2057, -0.0518,  0.7837,  0.2537,  1.6719, -0.0526, -0.0784,  0.2284],\n",
      "        [-1.4922,  0.7915,  1.0361, -0.1393,  0.4514, -2.1621,  0.6934, -0.7725],\n",
      "        [-0.5972,  1.7891,  0.4941, -0.3655, -0.6001,  0.0981, -1.4297, -0.4194],\n",
      "        [ 0.0776, -1.1885, -1.5986, -0.8301,  0.4568,  0.0557,  0.6499,  0.6655]],\n",
      "       device='cuda:0', dtype=torch.float16) tensor([[ 0.4968, -1.2080,  1.3506, -0.3557, -1.4297, -1.4785, -0.9575,  0.3882],\n",
      "        [ 0.0667, -0.8394,  0.8486,  0.4805,  1.1748,  1.3066,  0.2202,  0.1265],\n",
      "        [ 0.4814, -0.1096,  1.5107,  0.0319,  0.0923,  0.7266,  0.3655, -1.1816],\n",
      "        [-0.5835, -1.0488,  0.2805, -0.1309,  0.6543,  0.5708,  0.9922,  2.7246],\n",
      "        [-0.5269, -1.2871,  0.4556,  0.2390,  0.4377, -1.2969,  1.3955,  1.8174],\n",
      "        [-0.7871,  0.5479,  2.2910,  1.2939,  0.3333,  1.2773,  0.3081,  0.6152],\n",
      "        [ 1.8555, -0.7178, -0.0488, -0.1635,  0.8213,  0.0585, -0.6118, -0.3738],\n",
      "        [-2.0625, -0.2910, -1.2266, -0.5723,  1.5898,  0.4302, -1.0273, -0.7637]],\n",
      "       device='cuda:0', dtype=torch.float16) tensor([[-1.0439, -0.4873,  1.3848,  0.2698,  1.3564, -0.2299, -1.0186, -0.0699],\n",
      "        [-0.0813, -0.6138, -1.1846,  0.9761,  0.3918,  0.4795, -0.5532,  0.9219],\n",
      "        [-0.5361, -0.1324, -0.2064, -0.7563, -2.2637, -0.4636,  0.3230,  1.1621],\n",
      "        [-0.4912, -1.1748, -1.8457,  0.1483,  0.0263,  0.7212,  0.7188,  0.7412],\n",
      "        [-2.6172,  0.0183,  0.7861,  0.5635,  0.2874, -0.5503,  0.3203, -0.0906],\n",
      "        [-1.6943,  1.5928,  0.7363, -0.6694,  0.8262, -2.3984,  0.0978,  1.5244],\n",
      "        [-1.7090, -0.4163, -1.1895,  0.0925, -0.0956,  1.0391, -0.4504, -0.1678],\n",
      "        [ 1.1211,  0.1199, -1.3965, -0.8745,  0.0142, -1.1709,  1.5938, -1.6094],\n",
      "        [-2.0059,  0.5723,  0.2394, -1.9453,  0.8247, -0.9175,  0.3794, -0.4084],\n",
      "        [-0.0138,  0.3845, -0.3662, -1.2031,  0.1357,  0.1353,  0.8018,  0.7808],\n",
      "        [-1.8359,  0.5205,  0.2981, -0.3740, -0.6318,  1.1367,  1.0869, -0.2134],\n",
      "        [ 1.8965, -0.9346,  0.7290, -0.1610,  0.6060, -0.9590, -1.0967, -1.3750],\n",
      "        [-0.7168,  0.6382,  0.7300, -0.1488, -0.5225, -1.0088, -1.7910,  0.4451],\n",
      "        [-0.6450, -0.0859,  0.8120,  0.8828,  0.8076,  0.3486,  0.6289, -0.3252],\n",
      "        [-0.3918,  0.2644,  1.4443, -0.2155, -1.5625, -0.2238, -0.5444, -1.1113],\n",
      "        [ 2.0215,  0.5659, -0.3374, -0.9409, -0.2864,  0.3569, -0.3347, -1.3027]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "FP16矩阵乘加结果： tensor([[ 3.8125, -5.2969,  4.9219, -0.1140,  3.0586,  1.7725, -3.7617,  0.4429],\n",
      "        [ 4.7305, -1.2979,  1.1025,  0.5562, -4.8555, -3.8086,  0.8560, -0.6074],\n",
      "        [-0.7212,  0.2172,  0.3311, -1.0889, -3.6758, -0.3994, -1.4600,  0.7974],\n",
      "        [-2.2324, -3.5508, -2.2617, -1.2393,  1.2217,  1.0488, -2.2988,  0.4609],\n",
      "        [-0.9961, -2.1680,  1.1309,  0.7529,  2.4297, -1.2012,  1.7412,  0.1530],\n",
      "        [-1.1914,  0.5791,  1.6973, -0.9810,  4.6562,  1.6953,  1.8799,  0.6030],\n",
      "        [ 2.4199, -4.5352,  6.2812,  0.8003, -4.0000, -2.2949,  1.0996,  4.7500],\n",
      "        [ 6.0898,  0.0636, -1.1162, -0.3821, -1.6816, -2.4375,  4.8359, -1.5820],\n",
      "        [-2.1543,  0.3369, -2.6094, -4.1836,  1.1045, -1.1914, -4.6719, -5.2344],\n",
      "        [ 0.1680,  0.5791, -0.8765, -2.2500,  0.2151,  0.0698,  0.2329, -2.5039],\n",
      "        [-6.6797,  5.2695,  2.0117,  1.5684,  1.5674,  7.5664,  1.2344, -3.8438],\n",
      "        [ 2.3867,  1.5381,  2.4707,  1.7686,  0.8462,  0.4482, -3.4688, -5.0469],\n",
      "        [-1.8447, -2.1094,  2.5840, -0.0414,  0.3740, -2.8086,  0.6699,  3.1445],\n",
      "        [ 3.5898, -0.9536, -2.8398, -0.5156,  2.6934,  0.6235,  2.8066, -2.5859],\n",
      "        [-1.6670,  1.7881,  3.3359,  1.3770, -0.8716,  3.7871,  0.7378, -2.8711],\n",
      "        [ 1.2441,  1.2979, -4.3984, -1.7871, -0.6738, -3.1426, -2.5039, -1.6816]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[ 1.6221,  2.0879,  0.0956,  0.8218, -0.9561, -0.1516,  1.9365, -0.0624],\n",
      "        [ 0.8413, -0.0354,  1.0918, -0.9727,  0.6768, -1.2070, -0.5635, -1.8213],\n",
      "        [ 0.7544, -0.1205, -0.2488,  0.1873, -0.8584,  0.2036, -0.2035,  0.1149],\n",
      "        [ 1.2441,  0.8169, -0.2462,  0.4360, -0.6348, -0.2815,  0.0539,  1.3076],\n",
      "        [-0.3623,  0.5576,  0.4390, -0.3479,  1.2715, -0.3364,  0.9175, -0.0255],\n",
      "        [-1.0303,  0.4353,  2.0430,  1.0342, -0.1237, -0.3501,  0.6016,  0.4126],\n",
      "        [ 2.0996, -0.2260,  0.7910,  0.8428,  0.8516,  0.5503,  0.5938, -1.4502],\n",
      "        [-0.7832,  0.5044,  0.4968, -0.4678,  0.7192, -1.0879,  0.1075, -2.0059],\n",
      "        [ 0.9409,  0.2189, -0.1530, -0.5674, -1.2119, -0.7217,  0.4043,  1.3789],\n",
      "        [-0.0617, -0.2238,  1.2197, -0.3728, -0.0536, -0.6187, -0.1010,  0.4390],\n",
      "        [-1.3262, -0.0433,  1.0439, -0.1665, -1.2002,  1.5645, -1.2676,  0.8877],\n",
      "        [-0.0344,  0.1157, -0.6284, -1.5693, -0.3877,  1.5234,  0.6577,  0.1648],\n",
      "        [ 0.2057, -0.0518,  0.7837,  0.2537,  1.6719, -0.0526, -0.0784,  0.2284],\n",
      "        [-1.4922,  0.7915,  1.0361, -0.1393,  0.4514, -2.1621,  0.6934, -0.7725],\n",
      "        [-0.5972,  1.7891,  0.4941, -0.3655, -0.6001,  0.0981, -1.4297, -0.4194],\n",
      "        [ 0.0776, -1.1885, -1.5986, -0.8301,  0.4568,  0.0557,  0.6499,  0.6655]],\n",
      "       dtype=torch.float16) tensor([[ 0.4968, -1.2080,  1.3506, -0.3557, -1.4297, -1.4785, -0.9575,  0.3882],\n",
      "        [ 0.0667, -0.8394,  0.8486,  0.4805,  1.1748,  1.3066,  0.2202,  0.1265],\n",
      "        [ 0.4814, -0.1096,  1.5107,  0.0319,  0.0923,  0.7266,  0.3655, -1.1816],\n",
      "        [-0.5835, -1.0488,  0.2805, -0.1309,  0.6543,  0.5708,  0.9922,  2.7246],\n",
      "        [-0.5269, -1.2871,  0.4556,  0.2390,  0.4377, -1.2969,  1.3955,  1.8174],\n",
      "        [-0.7871,  0.5479,  2.2910,  1.2939,  0.3333,  1.2773,  0.3081,  0.6152],\n",
      "        [ 1.8555, -0.7178, -0.0488, -0.1635,  0.8213,  0.0585, -0.6118, -0.3738],\n",
      "        [-2.0625, -0.2910, -1.2266, -0.5723,  1.5898,  0.4302, -1.0273, -0.7637]],\n",
      "       dtype=torch.float16) tensor([[-1.0439, -0.4873,  1.3848,  0.2698,  1.3564, -0.2299, -1.0186, -0.0699],\n",
      "        [-0.0813, -0.6138, -1.1846,  0.9761,  0.3918,  0.4795, -0.5532,  0.9219],\n",
      "        [-0.5361, -0.1324, -0.2064, -0.7563, -2.2637, -0.4636,  0.3230,  1.1621],\n",
      "        [-0.4912, -1.1748, -1.8457,  0.1483,  0.0263,  0.7212,  0.7188,  0.7412],\n",
      "        [-2.6172,  0.0183,  0.7861,  0.5635,  0.2874, -0.5503,  0.3203, -0.0906],\n",
      "        [-1.6943,  1.5928,  0.7363, -0.6694,  0.8262, -2.3984,  0.0978,  1.5244],\n",
      "        [-1.7090, -0.4163, -1.1895,  0.0925, -0.0956,  1.0391, -0.4504, -0.1678],\n",
      "        [ 1.1211,  0.1199, -1.3965, -0.8745,  0.0142, -1.1709,  1.5938, -1.6094],\n",
      "        [-2.0059,  0.5723,  0.2394, -1.9453,  0.8247, -0.9175,  0.3794, -0.4084],\n",
      "        [-0.0138,  0.3845, -0.3662, -1.2031,  0.1357,  0.1353,  0.8018,  0.7808],\n",
      "        [-1.8359,  0.5205,  0.2981, -0.3740, -0.6318,  1.1367,  1.0869, -0.2134],\n",
      "        [ 1.8965, -0.9346,  0.7290, -0.1610,  0.6060, -0.9590, -1.0967, -1.3750],\n",
      "        [-0.7168,  0.6382,  0.7300, -0.1488, -0.5225, -1.0088, -1.7910,  0.4451],\n",
      "        [-0.6450, -0.0859,  0.8120,  0.8828,  0.8076,  0.3486,  0.6289, -0.3252],\n",
      "        [-0.3918,  0.2644,  1.4443, -0.2155, -1.5625, -0.2238, -0.5444, -1.1113],\n",
      "        [ 2.0215,  0.5659, -0.3374, -0.9409, -0.2864,  0.3569, -0.3347, -1.3027]],\n",
      "       dtype=torch.float16)\n",
      "FP16矩阵乘加结果： tensor([[ 3.8125, -5.2969,  4.9219, -0.1145,  3.0586,  1.7725, -3.7617,  0.4438],\n",
      "        [ 4.7305, -1.2979,  1.1025,  0.5552, -4.8594, -3.8086,  0.8560, -0.6055],\n",
      "        [-0.7212,  0.2174,  0.3311, -1.0889, -3.6777, -0.3994, -1.4590,  0.7979],\n",
      "        [-2.2324, -3.5508, -2.2578, -1.2402,  1.2207,  1.0488, -2.3008,  0.4624],\n",
      "        [-0.9961, -2.1680,  1.1309,  0.7524,  2.4316, -1.2031,  1.7402,  0.1522],\n",
      "        [-1.1914,  0.5781,  1.6934, -0.9814,  4.6562,  1.6953,  1.8809,  0.6045],\n",
      "        [ 2.4160, -4.5352,  6.2891,  0.8003, -4.0000, -2.2969,  1.1016,  4.7539],\n",
      "        [ 6.0898,  0.0632, -1.1172, -0.3823, -1.6816, -2.4395,  4.8359, -1.5801],\n",
      "        [-2.1543,  0.3364, -2.6113, -4.1836,  1.1035, -1.1924, -4.6719, -5.2305],\n",
      "        [ 0.1688,  0.5791, -0.8765, -2.2500,  0.2148,  0.0702,  0.2329, -2.5059],\n",
      "        [-6.6797,  5.2695,  2.0098,  1.5674,  1.5674,  7.5703,  1.2334, -3.8438],\n",
      "        [ 2.3867,  1.5361,  2.4707,  1.7695,  0.8467,  0.4492, -3.4688, -5.0469],\n",
      "        [-1.8447, -2.1094,  2.5820, -0.0413,  0.3740, -2.8086,  0.6719,  3.1445],\n",
      "        [ 3.5898, -0.9531, -2.8418, -0.5146,  2.6914,  0.6250,  2.8047, -2.5859],\n",
      "        [-1.6670,  1.7871,  3.3359,  1.3770, -0.8701,  3.7871,  0.7378, -2.8730],\n",
      "        [ 1.2441,  1.2969, -4.3984, -1.7871, -0.6758, -3.1426, -2.5059, -1.6826]],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def Mat_Mul(m: int, n: int, k: int,device:str):\n",
    "    # 确保你的矩阵是FP16类型\n",
    "    # A = torch.randn(m, k, dtype=torch.float16)\n",
    "    # B = torch.randn(k, n, dtype=torch.float16)\n",
    "    # C = torch.randn(m, n, dtype=torch.float16)\n",
    "\n",
    "    A = torch.tensor(np.loadtxt('a.txt'), dtype=torch.float16, device=device)\n",
    "    B = torch.tensor(np.loadtxt('b.txt'), dtype=torch.float16, device=device)\n",
    "    C = torch.tensor(np.loadtxt('c.txt'), dtype=torch.float16, device=device)\n",
    "    print(A,B,C)\n",
    "    # 执行FP16矩阵加法\n",
    "    D = torch.matmul(A, B) + C\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"FP16矩阵乘加结果：\", D)\n",
    "    np.savetxt('d_torch_'+device+'.txt', D.cpu().numpy())\n",
    "\n",
    "\n",
    "Mat_Mul(16, 8, 8,'cuda')\n",
    "Mat_Mul(16, 8, 8,'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b310fb18-c122-459c-9017-7f90fd952988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "dec_float = 5.9\n",
    "\n",
    "\n",
    "def getFP16Str(dec_float=5.9):\n",
    "    # # 十进制单精度浮点转16位16进制\n",
    "    hexa = struct.unpack('H', struct.pack('e', dec_float))[0]\n",
    "    # print()\n",
    "    hexa = hex(hexa)\n",
    "    hexa = hexa[2:]\n",
    "    # print(hexa) # 45e6\n",
    "    # print(dec_float.tobytes())\n",
    "    return hexa#str(dec_float.tobytes())#hexa\n",
    "\n",
    "\n",
    "def Hex2FP16(hexa: str):\n",
    "    y = struct.pack(\"H\", int(hexa, 16))\n",
    "    float = np.frombuffer(y, dtype=np.float16)[0]\n",
    "    print(float)  # 5.9\n",
    "    return float\n",
    "\n",
    "\n",
    "def MatFP16ToHexString(a):\n",
    "    a_str = []\n",
    "    for i in a:\n",
    "        for j in i:\n",
    "            a_str.append(getFP16Str(j))\n",
    "    return a_str\n",
    "\n",
    "\n",
    "def MatABCD2Register(a, b, c, d, thread: int = 32):\n",
    "    Register_A = ['h' for i in range(thread)]\n",
    "    Register_B = ['h' for i in range(thread)]\n",
    "    Register_C = ['h' for i in range(thread)]\n",
    "    Register_D = ['h' for i in range(thread)]\n",
    "    for i in range(thread):\n",
    "        Register_A[i] += (a[2 * i + 1 + 64] + a[2 * i + 64] + a[2 * i + 1] + a[2 * i])\n",
    "        Register_B[i] += ('0000' + '0000' + b[2 * i + 1] + b[2 * i])\n",
    "        Register_C[i] += (c[2 * i + 1 + 64] + c[2 * i + 64] + c[2 * i + 1] + c[2 * i])\n",
    "        Register_D[i] += (d[2 * i + 1 + 64] + d[2 * i + 64] + d[2 * i + 1] + d[2 * i])\n",
    "\n",
    "    return Register_A, Register_B, Register_C, Register_D\n",
    "\n",
    "\n",
    "def getMatABCD(m: int, n: int, k: int):\n",
    "    a = np.random.randn(m, k).astype(np.float16)\n",
    "    b = np.random.randn(k, n).astype(np.float16)\n",
    "    c = np.random.randn(m, n).astype(np.float16)\n",
    "\n",
    "    d = np.dot(a, b) + c\n",
    "    print(d.dtype, d)\n",
    "    np.savetxt('testData/a.txt', a)\n",
    "    np.savetxt('testData/b.txt', b)\n",
    "    np.savetxt('testData/c.txt', c)\n",
    "    np.savetxt('testData/d.txt', d)\n",
    "\n",
    "    print('Save Done!')\n",
    "    return a, b, c, d\n",
    "\n",
    "\n",
    "def strArr_Save(str_array, fname: str):\n",
    "    # 使用空格将数组中的字符串连接起来\n",
    "    space_separated_string = ' '.join(str_array)\n",
    "    # 将连接后的字符串写入到TXT文件中\n",
    "    with open(fname, 'w', encoding='utf-8') as file:\n",
    "        file.write(space_separated_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba848eb-0111-4de6-9371-be2868e9eb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# a, b, c, d = getMatABCD(16, 8, 8)\n",
    "# print(getFP16Str(0))\n",
    "a = np.loadtxt('a.txt')\n",
    "b = np.loadtxt('b.txt')\n",
    "c = np.loadtxt('c.txt')\n",
    "# d = np.dot(a, b) + c# np.loadtxt('testData/d.txt')\n",
    "# np.savetxt('testData/d.txt',d)\n",
    "# print(np.dot(a, b) + c)\n",
    "\n",
    "d = np.loadtxt('d_torch_cuda.txt')\n",
    "\n",
    "a_str = MatFP16ToHexString(a)\n",
    "b_str = MatFP16ToHexString(b.T)\n",
    "c_str = MatFP16ToHexString(c)\n",
    "d_str = MatFP16ToHexString(d)\n",
    "# print(a_str)\n",
    "print(len(a_str))\n",
    "RA, RB, RC, RD = MatABCD2Register(a_str, b_str, c_str, d_str)\n",
    "# strArr_Save(RA, 'RA.txt')\n",
    "# strArr_Save(RB, 'RB.txt')\n",
    "# strArr_Save(RC, 'RC.txt')\n",
    "strArr_Save(RD, 'RD_torch_cuda.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e75cdfcb-77cc-4b50-ad9c-aa348cd77a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# a, b, c, d = getMatABCD(16, 8, 8)\n",
    "# print(getFP16Str(0))\n",
    "a = np.loadtxt('a.txt')\n",
    "b = np.loadtxt('b.txt')\n",
    "c = np.loadtxt('c.txt')\n",
    "# d = np.dot(a, b) + c# np.loadtxt('testData/d.txt')\n",
    "# np.savetxt('testData/d.txt',d)\n",
    "# print(np.dot(a, b) + c)\n",
    "\n",
    "d = np.loadtxt('d_torch_cpu.txt')\n",
    "\n",
    "a_str = MatFP16ToHexString(a)\n",
    "b_str = MatFP16ToHexString(b.T)\n",
    "c_str = MatFP16ToHexString(c)\n",
    "d_str = MatFP16ToHexString(d)\n",
    "# print(a_str)\n",
    "print(len(a_str))\n",
    "RA, RB, RC, RD = MatABCD2Register(a_str, b_str, c_str, d_str)\n",
    "# strArr_Save(RA, 'RA.txt')\n",
    "# strArr_Save(RB, 'RB.txt')\n",
    "# strArr_Save(RC, 'RC.txt')\n",
    "strArr_Save(RD, 'RD_torch_cpu.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab17ed-f561-4542-a960-26c7e4cca8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.8",
   "language": "python",
   "name": "torch1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
